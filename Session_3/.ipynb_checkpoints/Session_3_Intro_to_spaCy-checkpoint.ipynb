{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction to \n",
    "<img src=\"https://miro.medium.com/max/1200/1*HTtQseukwrBiREJf8MSVcA.jpeg\" alt=\"Spacy Logo\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "- [Main Documentation Page](https://spacy.io/)  \n",
    "- [How to install spaCy](https://spacy.io/usage)\n",
    "- [spaCy 101, The most important concepts, explained in simple terms\n",
    "](https://spacy.io/usage/spacy-101)\n",
    "- [Free course- Advanced NLP with spaCy](https://course.spacy.io/)\n",
    "\n",
    "### Without spaCy, Python is able to process text as a sequence of characters (called a string).  We can slice a string, we can add strings, replace sections of a string and many other tasks.  \n",
    "\n",
    "See [w3schools string functions](https://www.w3schools.com/python/python_ref_string.asp)\n",
    "\n",
    "Common examples for working with strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting a slice, selecting part of the string from [begin : end]\n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "print('the string \"{}\" has {} characters. Note that the index begins at 0.'.format(wilde, len(wilde)))\n",
    "\n",
    "[print(i, charachter) for i, charachter in enumerate(wilde)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('wilde[4:12] will start at position 4 and end at 12 ->', wilde[4:12])\n",
    "print('or read backwards from the end [-40: -32] ->', wilde[-40:-32])\n",
    "print('you can even mix forward and backward! wilde[-40:12]', wilde[-40:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find and replace\n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "wilde.replace('yourself', 'a fish').replace('everyone', 'everything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split \n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "print(wilde.split()) # Split on empty spaces\n",
    "print(wilde.split(';'))\n",
    "print(wilde.split('y'))  #Note that the charachter or space used to split the string is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can join a list of strings\n",
    "print(' '.join(['Be', 'yourself;', 'everyone', 'else', 'is', 'already', 'taken.']))\n",
    "\n",
    "import random\n",
    "\n",
    "animals = ['fish', 'turtle', 'panther', 'parrot']\n",
    "adjective = ['scary', 'green', 'overweight', 'fluffy']\n",
    "print('Be a ' + ' '.join([random.choice(adjective), random.choice(animals)]) + ' everything else is taken.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy gives the machine an understanding of text, not just as a sequence of characters, but as natural language\n",
    "\n",
    "[A full list of base languages](https://github.com/explosion/spaCy/tree/master/spacy/lang)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n",
    "\n",
    "nlp = German()\n",
    "doc = nlp('Sei du selbst! Alle anderen sind bereits vergeben.')\n",
    "\n",
    "\n",
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "doc = nlp('Be yourself; everyone else is already taken.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The document object\n",
    "Once we have imported a base language class or language model and a text, spaCy will create what is called a document (doc) object.  \n",
    "The doc object typically contains:\n",
    "\n",
    "\n",
    "|   [attributes](https://spacy.io/api/doc#attributes) |   | \n",
    "|---|---|\n",
    "| tokens (individual parts of the text)  | doc[5]  |\n",
    "|  the text  | doc.text\n",
    "| the text split into sentences  | doc.sents |\n",
    "| entities detected in the text | doc.ents |\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/doc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Note the difference between working with a slice of a doc object versus a Python string**\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wilde' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9372f16622c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'**Note the difference between working with a slice of a doc object versus a Python string**'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwilde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wilde' is not defined"
     ]
    }
   ],
   "source": [
    "print('**Note the difference between working with a slice of a doc object versus a Python string**')\n",
    "\n",
    "print(wilde[:3])\n",
    "print(doc[:3])\n",
    "\n",
    "print('**Also note how spaCy tokenization differs from Python split()**')\n",
    "print('[*] Python:')\n",
    "for token in wilde.split():\n",
    "    print(token)\n",
    "    \n",
    "print('------')    \n",
    "print('[*] spaCy:')\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\\nIf you‚Äôre working with a lot of text, you‚Äôll eventually want to know more about it. For example, what‚Äôs it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\\nspaCy is designed specifically for production use and helps you build applications that process and ‚Äúunderstand‚Äù large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning..',\n",
       " 'tokens': [{'id': 0, 'start': 0, 'end': 5},\n",
       "  {'id': 1, 'start': 6, 'end': 8},\n",
       "  {'id': 2, 'start': 9, 'end': 10},\n",
       "  {'id': 3, 'start': 11, 'end': 15},\n",
       "  {'id': 4, 'start': 15, 'end': 16},\n",
       "  {'id': 5, 'start': 17, 'end': 21},\n",
       "  {'id': 6, 'start': 21, 'end': 22},\n",
       "  {'id': 7, 'start': 22, 'end': 28},\n",
       "  {'id': 8, 'start': 29, 'end': 36},\n",
       "  {'id': 9, 'start': 37, 'end': 40},\n",
       "  {'id': 10, 'start': 41, 'end': 49},\n",
       "  {'id': 11, 'start': 50, 'end': 57},\n",
       "  {'id': 12, 'start': 58, 'end': 66},\n",
       "  {'id': 13, 'start': 67, 'end': 77},\n",
       "  {'id': 14, 'start': 78, 'end': 79},\n",
       "  {'id': 15, 'start': 79, 'end': 82},\n",
       "  {'id': 16, 'start': 82, 'end': 83},\n",
       "  {'id': 17, 'start': 84, 'end': 86},\n",
       "  {'id': 18, 'start': 87, 'end': 93},\n",
       "  {'id': 19, 'start': 93, 'end': 94},\n",
       "  {'id': 20, 'start': 94, 'end': 95},\n",
       "  {'id': 21, 'start': 95, 'end': 97},\n",
       "  {'id': 22, 'start': 98, 'end': 101},\n",
       "  {'id': 23, 'start': 101, 'end': 104},\n",
       "  {'id': 24, 'start': 105, 'end': 112},\n",
       "  {'id': 25, 'start': 113, 'end': 117},\n",
       "  {'id': 26, 'start': 118, 'end': 119},\n",
       "  {'id': 27, 'start': 120, 'end': 123},\n",
       "  {'id': 28, 'start': 124, 'end': 126},\n",
       "  {'id': 29, 'start': 127, 'end': 131},\n",
       "  {'id': 30, 'start': 131, 'end': 132},\n",
       "  {'id': 31, 'start': 133, 'end': 136},\n",
       "  {'id': 32, 'start': 136, 'end': 139},\n",
       "  {'id': 33, 'start': 140, 'end': 150},\n",
       "  {'id': 34, 'start': 151, 'end': 155},\n",
       "  {'id': 35, 'start': 156, 'end': 158},\n",
       "  {'id': 36, 'start': 159, 'end': 163},\n",
       "  {'id': 37, 'start': 164, 'end': 168},\n",
       "  {'id': 38, 'start': 169, 'end': 174},\n",
       "  {'id': 39, 'start': 175, 'end': 177},\n",
       "  {'id': 40, 'start': 177, 'end': 178},\n",
       "  {'id': 41, 'start': 179, 'end': 182},\n",
       "  {'id': 42, 'start': 183, 'end': 190},\n",
       "  {'id': 43, 'start': 190, 'end': 191},\n",
       "  {'id': 44, 'start': 192, 'end': 196},\n",
       "  {'id': 45, 'start': 196, 'end': 198},\n",
       "  {'id': 46, 'start': 199, 'end': 201},\n",
       "  {'id': 47, 'start': 202, 'end': 207},\n",
       "  {'id': 48, 'start': 207, 'end': 208},\n",
       "  {'id': 49, 'start': 209, 'end': 213},\n",
       "  {'id': 50, 'start': 214, 'end': 216},\n",
       "  {'id': 51, 'start': 217, 'end': 220},\n",
       "  {'id': 52, 'start': 221, 'end': 226},\n",
       "  {'id': 53, 'start': 227, 'end': 231},\n",
       "  {'id': 54, 'start': 232, 'end': 234},\n",
       "  {'id': 55, 'start': 235, 'end': 242},\n",
       "  {'id': 56, 'start': 242, 'end': 243},\n",
       "  {'id': 57, 'start': 244, 'end': 247},\n",
       "  {'id': 58, 'start': 248, 'end': 250},\n",
       "  {'id': 59, 'start': 251, 'end': 256},\n",
       "  {'id': 60, 'start': 257, 'end': 261},\n",
       "  {'id': 61, 'start': 262, 'end': 264},\n",
       "  {'id': 62, 'start': 265, 'end': 269},\n",
       "  {'id': 63, 'start': 269, 'end': 270},\n",
       "  {'id': 64, 'start': 271, 'end': 275},\n",
       "  {'id': 65, 'start': 276, 'end': 285},\n",
       "  {'id': 66, 'start': 286, 'end': 289},\n",
       "  {'id': 67, 'start': 290, 'end': 298},\n",
       "  {'id': 68, 'start': 299, 'end': 302},\n",
       "  {'id': 69, 'start': 303, 'end': 312},\n",
       "  {'id': 70, 'start': 312, 'end': 313},\n",
       "  {'id': 71, 'start': 314, 'end': 319},\n",
       "  {'id': 72, 'start': 320, 'end': 325},\n",
       "  {'id': 73, 'start': 326, 'end': 329},\n",
       "  {'id': 74, 'start': 330, 'end': 337},\n",
       "  {'id': 75, 'start': 338, 'end': 340},\n",
       "  {'id': 76, 'start': 341, 'end': 345},\n",
       "  {'id': 77, 'start': 346, 'end': 351},\n",
       "  {'id': 78, 'start': 351, 'end': 352},\n",
       "  {'id': 79, 'start': 352, 'end': 353},\n",
       "  {'id': 80, 'start': 353, 'end': 358},\n",
       "  {'id': 81, 'start': 359, 'end': 361},\n",
       "  {'id': 82, 'start': 362, 'end': 370},\n",
       "  {'id': 83, 'start': 371, 'end': 383},\n",
       "  {'id': 84, 'start': 384, 'end': 387},\n",
       "  {'id': 85, 'start': 388, 'end': 398},\n",
       "  {'id': 86, 'start': 399, 'end': 402},\n",
       "  {'id': 87, 'start': 403, 'end': 406},\n",
       "  {'id': 88, 'start': 407, 'end': 412},\n",
       "  {'id': 89, 'start': 413, 'end': 416},\n",
       "  {'id': 90, 'start': 417, 'end': 422},\n",
       "  {'id': 91, 'start': 423, 'end': 435},\n",
       "  {'id': 92, 'start': 436, 'end': 440},\n",
       "  {'id': 93, 'start': 441, 'end': 448},\n",
       "  {'id': 94, 'start': 449, 'end': 452},\n",
       "  {'id': 95, 'start': 453, 'end': 454},\n",
       "  {'id': 96, 'start': 454, 'end': 464},\n",
       "  {'id': 97, 'start': 464, 'end': 465},\n",
       "  {'id': 98, 'start': 466, 'end': 471},\n",
       "  {'id': 99, 'start': 472, 'end': 479},\n",
       "  {'id': 100, 'start': 480, 'end': 482},\n",
       "  {'id': 101, 'start': 483, 'end': 487},\n",
       "  {'id': 102, 'start': 487, 'end': 488},\n",
       "  {'id': 103, 'start': 489, 'end': 491},\n",
       "  {'id': 104, 'start': 492, 'end': 495},\n",
       "  {'id': 105, 'start': 496, 'end': 498},\n",
       "  {'id': 106, 'start': 499, 'end': 503},\n",
       "  {'id': 107, 'start': 504, 'end': 506},\n",
       "  {'id': 108, 'start': 507, 'end': 512},\n",
       "  {'id': 109, 'start': 513, 'end': 524},\n",
       "  {'id': 110, 'start': 525, 'end': 535},\n",
       "  {'id': 111, 'start': 536, 'end': 538},\n",
       "  {'id': 112, 'start': 539, 'end': 546},\n",
       "  {'id': 113, 'start': 547, 'end': 555},\n",
       "  {'id': 114, 'start': 556, 'end': 569},\n",
       "  {'id': 115, 'start': 570, 'end': 577},\n",
       "  {'id': 116, 'start': 577, 'end': 578},\n",
       "  {'id': 117, 'start': 579, 'end': 581},\n",
       "  {'id': 118, 'start': 582, 'end': 584},\n",
       "  {'id': 119, 'start': 585, 'end': 588},\n",
       "  {'id': 120, 'start': 588, 'end': 589},\n",
       "  {'id': 121, 'start': 589, 'end': 596},\n",
       "  {'id': 122, 'start': 597, 'end': 601},\n",
       "  {'id': 123, 'start': 602, 'end': 605},\n",
       "  {'id': 124, 'start': 606, 'end': 610},\n",
       "  {'id': 125, 'start': 611, 'end': 619},\n",
       "  {'id': 126, 'start': 619, 'end': 621}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The to_json() method is a useful way to view the information contained in the doc object\n",
    "\n",
    "doc = nlp('''spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "If you‚Äôre working with a lot of text, you‚Äôll eventually want to know more about it. For example, what‚Äôs it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n",
    "spaCy is designed specifically for production use and helps you build applications that process and ‚Äúunderstand‚Äù large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning..''')\n",
    "\n",
    "doc.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Tokens\n",
    "As you can see above, the doc contains a split of the text into tokens.  Each token object has [65 attributes](https://spacy.io/api/token#attributes) that can be used during analysis.  Common tasks include:\n",
    "- removing all punctuation from the text\n",
    "- counting root forms of the words (lemmata)\n",
    "- removing stopwords from the doc\n",
    "\n",
    "\n",
    "|   [attributes](https://spacy.io/api/token#attributes) |   | \n",
    "|---|---|\n",
    "| root form (lemma)  | token.lemma_  |\n",
    "| Named entity type  | token.ent_type_ |\n",
    "| token is punctuation  | token.is_punct |\n",
    "| part of speech | token.pos_ |\n",
    "| in stop words | token.is_stop |\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/token#_title).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\n",
    "         token.lemma_,\n",
    "         token.pos_,\n",
    "         token.dep_,\n",
    "         token.shape_,\n",
    "         token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function to make sense of linguistic terminology and abbreviations \n",
    "import spacy \n",
    "\n",
    "spacy.explain(\"PRON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ru_model0')\n",
    "text = \"\"\"–Ø, –§—Ä–∞–Ω—Å—É–∞ –í–∏–π–æ–Ω, —à–∫–æ–ª—è—Ä, \n",
    "         –í —Å–µ–º –ø—è—Ç—å–¥–µ—Å—è—Ç —à–µ—Å—Ç–æ–º –≥–æ–¥—É, \n",
    "         –ü–æ–æ—Å—Ç—É–¥–∏–≤ —Å–µ—Ä–¥–µ—á–Ω—ã–π –∂–∞—Ä, \n",
    "         –ò –Ω–∞–ª–æ–∂–∏–≤ –Ω–∞ –º—ã—Å–ª—å —É–∑–¥—É, \n",
    "         –ò –∑–Ω–∞—è, —á—Ç–æ –∫ –∫–æ–Ω—Ü—É –∏–¥—É, \n",
    "         –ù–∞—à–µ–ª, —á—Ç–æ –≤—Ä–µ–º—è –ø—Ä–∏–≥–ª—è–¥–µ—Ç—å—Å—è \n",
    "         –ö —Å–µ–±–µ –∏ —Å–≤–æ–µ–º—É —Ç—Ä—É–¥—É, \n",
    "         –ö–∞–∫ —É—á–∏—Ç —Ä–∏–º–ª—è–Ω–∏–Ω –í–µ–≥–µ—Ü–∏–π.\"\"\" \n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('_', <spacy.tokens.underscore.Underscore object at 0x7f2843276310>)\n",
      "('__class__', <class 'spacy.tokens.token.Token'>)\n",
      "('__delattr__', <method-wrapper '__delattr__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__doc__', 'An individual token ‚Äì i.e. a word, punctuation symbol, whitespace,\\n    etc.\\n\\n    DOCS: https://spacy.io/api/token\\n    ')\n",
      "('__eq__', <method-wrapper '__eq__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__ge__', <method-wrapper '__ge__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__getattribute__', <method-wrapper '__getattribute__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__gt__', <method-wrapper '__gt__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__hash__', <method-wrapper '__hash__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__init__', <method-wrapper '__init__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__le__', <method-wrapper '__le__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__len__', <method-wrapper '__len__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__lt__', <method-wrapper '__lt__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__ne__', <method-wrapper '__ne__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__pyx_vtable__', <capsule object NULL at 0x7f2846ee91b0>)\n",
      "('__repr__', <method-wrapper '__repr__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__setattr__', <method-wrapper '__setattr__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('__str__', <method-wrapper '__str__' of spacy.tokens.token.Token object at 0x7f284324c410>)\n",
      "('ancestors', <generator object at 0x7f28438f6910>)\n",
      "('children', <generator object at 0x7f2843243870>)\n",
      "('cluster', 0)\n",
      "('conjuncts', (—à–∫–æ–ª—è—Ä, –≥–æ–¥—É, –∂–∞—Ä, –Ω–∞–ª–æ–∂–∏–≤))\n",
      "('dep', 429)\n",
      "('dep_', 'nsubj')\n",
      "('doc', –Ø, –§—Ä–∞–Ω—Å—É–∞ –í–∏–π–æ–Ω, —à–∫–æ–ª—è—Ä, \n",
      "         –í —Å–µ–º –ø—è—Ç—å–¥–µ—Å—è—Ç —à–µ—Å—Ç–æ–º –≥–æ–¥—É, \n",
      "         –ü–æ–æ—Å—Ç—É–¥–∏–≤ —Å–µ—Ä–¥–µ—á–Ω—ã–π –∂–∞—Ä, \n",
      "         –ò –Ω–∞–ª–æ–∂–∏–≤ –Ω–∞ –º—ã—Å–ª—å —É–∑–¥—É, \n",
      "         –ò –∑–Ω–∞—è, —á—Ç–æ –∫ –∫–æ–Ω—Ü—É –∏–¥—É, \n",
      "         –ù–∞—à–µ–ª, —á—Ç–æ –≤—Ä–µ–º—è –ø—Ä–∏–≥–ª—è–¥–µ—Ç—å—Å—è \n",
      "         –ö —Å–µ–±–µ –∏ —Å–≤–æ–µ–º—É —Ç—Ä—É–¥—É, \n",
      "         –ö–∞–∫ —É—á–∏—Ç —Ä–∏–º–ª—è–Ω–∏–Ω –í–µ–≥–µ—Ü–∏–π.)\n",
      "('ent_id', 0)\n",
      "('ent_id_', '')\n",
      "('ent_iob', 2)\n",
      "('ent_iob_', 'O')\n",
      "('ent_kb_id', 0)\n",
      "('ent_kb_id_', '')\n",
      "('ent_type', 0)\n",
      "('ent_type_', '')\n",
      "('has_vector', True)\n",
      "('head', –∑–Ω–∞—è)\n",
      "('i', 0)\n",
      "('idx', 0)\n",
      "('is_alpha', True)\n",
      "('is_ascii', False)\n",
      "('is_bracket', False)\n",
      "('is_currency', False)\n",
      "('is_digit', False)\n",
      "('is_left_punct', False)\n",
      "('is_lower', False)\n",
      "('is_oov', True)\n",
      "('is_punct', False)\n",
      "('is_quote', False)\n",
      "('is_right_punct', False)\n",
      "('is_sent_start', True)\n",
      "('is_space', False)\n",
      "('is_stop', True)\n",
      "('is_title', True)\n",
      "('is_upper', True)\n",
      "('lang', 9775329161392182269)\n",
      "('lang_', 'ru')\n",
      "('left_edge', –Ø)\n",
      "('lefts', <generator object at 0x7f2842b55af0>)\n",
      "('lemma', 12329462409828574123)\n",
      "('lemma_', '—è')\n",
      "('lex_id', 0)\n",
      "('like_email', False)\n",
      "('like_num', False)\n",
      "('like_url', False)\n",
      "('lower', 12329462409828574123)\n",
      "('lower_', '—è')\n",
      "__repr__ returned non-string (type list)\n",
      "('n_lefts', 0)\n",
      "('n_rights', 7)\n",
      "('norm', 12329462409828574123)\n",
      "('norm_', '—è')\n",
      "('orth', 17850992064964131792)\n",
      "('orth_', '–Ø')\n",
      "('pos', 95)\n",
      "('pos_', 'PRON')\n",
      "('prefix', 17850992064964131792)\n",
      "('prefix_', '–Ø')\n",
      "('prob', -20.0)\n",
      "('rank', 0)\n",
      "('right_edge', —É–∑–¥—É)\n",
      "('rights', <generator object at 0x7f288807b410>)\n",
      "('sent', –Ø, –§—Ä–∞–Ω—Å—É–∞ –í–∏–π–æ–Ω, —à–∫–æ–ª—è—Ä, \n",
      "         –í —Å–µ–º –ø—è—Ç—å–¥–µ—Å—è—Ç —à–µ—Å—Ç–æ–º –≥–æ–¥—É, \n",
      "         –ü–æ–æ—Å—Ç—É–¥–∏–≤ —Å–µ—Ä–¥–µ—á–Ω—ã–π –∂–∞—Ä, \n",
      "         –ò –Ω–∞–ª–æ–∂–∏–≤ –Ω–∞ –º—ã—Å–ª—å —É–∑–¥—É, \n",
      "         –ò –∑–Ω–∞—è, —á—Ç–æ –∫ –∫–æ–Ω—Ü—É –∏–¥—É, \n",
      "         –ù–∞—à–µ–ª, —á—Ç–æ –≤—Ä–µ–º—è –ø—Ä–∏–≥–ª—è–¥–µ—Ç—å—Å—è \n",
      "         –ö —Å–µ–±–µ –∏ —Å–≤–æ–µ–º—É —Ç—Ä—É–¥—É, \n",
      "         –ö–∞–∫ —É—á–∏—Ç —Ä–∏–º–ª—è–Ω–∏–Ω –í–µ–≥–µ—Ü–∏–π.)\n",
      "('sent_start', False)\n",
      "('sentiment', 0.0)\n",
      "('shape', 101)\n",
      "('shape_', 'X')\n",
      "('string', '–Ø')\n",
      "('subtree', <generator object at 0x7f288807b7d0>)\n",
      "('suffix', 17850992064964131792)\n",
      "('suffix_', '–Ø')\n",
      "('tag', 95)\n",
      "('tag_', 'PRON')\n",
      "('tensor', array([-2.9259772 , -3.1861346 ,  0.7641392 , -1.0292368 , -2.5404344 ,\n",
      "       -1.105762  , -1.9642249 ,  3.3661537 ,  0.0373531 ,  2.7276726 ,\n",
      "       -0.05869108, -0.36729914, -2.6534781 ,  1.9625474 , -1.1813787 ,\n",
      "        1.6391444 , -2.032161  , -0.18790582, -2.4885378 ,  3.5222037 ,\n",
      "        0.04684146, -1.6021919 , -0.24613619,  5.3993316 ,  2.1223626 ,\n",
      "       -0.44736052, -1.905368  , -0.6352039 , -0.5099405 , -0.71999836,\n",
      "        0.5202686 , -1.2914221 ,  0.9363191 , -0.13492513, -2.5197783 ,\n",
      "        1.840367  , -0.14195573,  1.0539981 ,  2.9140685 ,  0.75018966,\n",
      "       -1.4705899 , -2.585985  ,  4.8978233 , -1.6674403 ,  2.8071287 ,\n",
      "       -1.5018466 ,  1.2416275 ,  1.4582746 ,  2.6575441 , -1.9065413 ,\n",
      "       -1.7522568 ,  1.302942  , -0.43893993,  0.79438305, -0.990229  ,\n",
      "        2.5661888 , -4.2256527 , -4.1402063 ,  5.8194933 ,  0.43986893,\n",
      "       -0.1735487 , -1.5478445 ,  1.8693153 ,  3.67177   ,  1.3463401 ,\n",
      "       -2.3031845 , -2.0776842 , -2.512003  , -1.67476   , -2.1849442 ,\n",
      "       -0.07267976, -2.674073  , -1.1997308 ,  0.76558816,  2.1104288 ,\n",
      "       -2.0289626 , -3.3186076 ,  0.1310059 ,  6.8660493 ,  1.9024787 ,\n",
      "        2.3220959 , -1.5227615 , -2.6420333 , -0.9329869 ,  0.13785464,\n",
      "       -1.0028203 , -1.5958123 ,  3.3501527 ,  3.0556805 , -3.4442265 ,\n",
      "        2.294901  ,  1.570817  , -1.0386901 ,  4.4964304 , -2.8904433 ,\n",
      "        2.509877  ], dtype=float32))\n",
      "('text', '–Ø')\n",
      "('text_with_ws', '–Ø')\n",
      "('vector', array([-2.9259772 , -3.1861346 ,  0.7641392 , -1.0292368 , -2.5404344 ,\n",
      "       -1.105762  , -1.9642249 ,  3.3661537 ,  0.0373531 ,  2.7276726 ,\n",
      "       -0.05869108, -0.36729914, -2.6534781 ,  1.9625474 , -1.1813787 ,\n",
      "        1.6391444 , -2.032161  , -0.18790582, -2.4885378 ,  3.5222037 ,\n",
      "        0.04684146, -1.6021919 , -0.24613619,  5.3993316 ,  2.1223626 ,\n",
      "       -0.44736052, -1.905368  , -0.6352039 , -0.5099405 , -0.71999836,\n",
      "        0.5202686 , -1.2914221 ,  0.9363191 , -0.13492513, -2.5197783 ,\n",
      "        1.840367  , -0.14195573,  1.0539981 ,  2.9140685 ,  0.75018966,\n",
      "       -1.4705899 , -2.585985  ,  4.8978233 , -1.6674403 ,  2.8071287 ,\n",
      "       -1.5018466 ,  1.2416275 ,  1.4582746 ,  2.6575441 , -1.9065413 ,\n",
      "       -1.7522568 ,  1.302942  , -0.43893993,  0.79438305, -0.990229  ,\n",
      "        2.5661888 , -4.2256527 , -4.1402063 ,  5.8194933 ,  0.43986893,\n",
      "       -0.1735487 , -1.5478445 ,  1.8693153 ,  3.67177   ,  1.3463401 ,\n",
      "       -2.3031845 , -2.0776842 , -2.512003  , -1.67476   , -2.1849442 ,\n",
      "       -0.07267976, -2.674073  , -1.1997308 ,  0.76558816,  2.1104288 ,\n",
      "       -2.0289626 , -3.3186076 ,  0.1310059 ,  6.8660493 ,  1.9024787 ,\n",
      "        2.3220959 , -1.5227615 , -2.6420333 , -0.9329869 ,  0.13785464,\n",
      "       -1.0028203 , -1.5958123 ,  3.3501527 ,  3.0556805 , -3.4442265 ,\n",
      "        2.294901  ,  1.570817  , -1.0386901 ,  4.4964304 , -2.8904433 ,\n",
      "        2.509877  ], dtype=float32))\n",
      "('vector_norm', 22.681297)\n",
      "('vocab', <spacy.vocab.Vocab object at 0x7f2842f6db00>)\n",
      "('whitespace_', '')\n"
     ]
    }
   ],
   "source": [
    "# This cell will print all of the attributes of a token.  Change the index to inspect difference tokens in the doc.\n",
    "\n",
    "token_index = 0\n",
    "\n",
    "import inspect\n",
    "import random\n",
    "\n",
    "attributes = inspect.getmembers(doc[token_index], lambda a:not(inspect.isroutine(a)))\n",
    "output = ''\n",
    "for attribute in attributes:\n",
    "    try:\n",
    "        print(attribute)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Spans\n",
    "When studying text, we are often interested in features that involve more than one token.  To do this, we can create a span.  For example, \"New York City\"\n",
    "\n",
    "Span [attributes](https://spacy.io/api/span#attributes)\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/span#_title). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I just got back from New York City.'\n",
    "nlp = English()\n",
    "doc = nlp(text)\n",
    "\n",
    "nyc = doc[5:8] \n",
    "\n",
    "print(\n",
    "    '[*] spaCy',\n",
    "    nyc.start,\n",
    "    nyc.end,\n",
    "    doc[nyc.start:nyc.end],\n",
    ")\n",
    "print(  \n",
    "    '[*] string',\n",
    "    nyc.start_char,\n",
    "    nyc.end_char,\n",
    "    text[nyc.start_char:nyc.end_char]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: create individualized vocabularly lists \n",
    "At Haverford, we have an application called [the Bridge](https://bridge.haverford.edu/) that generates custom vocabulary lists for learning Latin and ancient Greek.  To do this, we create a list of words from texts that the student has already read and understood.  We then use the lemma of each word to compare the list of known words against words in a new text.  We can then identify which words will be new to the reader.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/J%C3%B3kai_M%C3%B3r_litogr%C3%A1fia.jpg/220px-J%C3%B3kai_M%C3%B3r_litogr%C3%A1fia.jpg'>\n",
    "\n",
    "For current purposes, let's use texts in Hungarian. Let's say that I'm learning Hungarian and reading M√≥r J√≥kai's *The novel of the next century* (1872).  I have just finished book one and want to know what new words I will encounter when reading book two.   \n",
    "\n",
    ">*Note* I am using Python sets to find the difference between the two books. I could also find the union, the intersection and other set operations.  For more on this topic, there is an excellent tutorial from [Real Python](https://realpython.com/python-sets/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I use the requests library to get the texts from Project Gutenberg\n",
    "import requests \n",
    "vol_1 = requests.get('http://www.gutenberg.org/files/55911/55911-0.txt')\n",
    "vol_2 = requests.get('http://www.gutenberg.org/files/55912/55912-0.txt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.hu import Hungarian \n",
    "nlp = Hungarian()\n",
    "nlp.max_length = 1070000 # This is needed given the length of the text \n",
    "\n",
    "vol_1_doc = nlp(vol_1.text)\n",
    "\n",
    "# Create a set of words that are not punctuation or stop words\n",
    "vol_1_words = set([token.lemma_ for token in vol_1_doc if token.is_stop is False and token.is_punct is False])\n",
    "\n",
    "vol_2_doc = nlp(vol_2.text)\n",
    "vol_2_words = set([token.lemma_ for token in vol_2_doc if token.is_stop is False and token.is_punct is False])\n",
    "\n",
    "new_words = vol_2_words.difference(vol_1_words)\n",
    "len(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouch, that's far too many words to learn!  Let's only count the 100 most frequent words and then create our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "from collections import Counter\n",
    "\n",
    "# Add an extension to our tokens called \"count\"\n",
    "Token.set_extension(\"count\", default=False, force=True)\n",
    "\n",
    "\n",
    "# Calculate the number of times that a lemma appears in the text\n",
    "counts = Counter([token.lemma_ for token in vol_1_doc if not token.is_punct and not token.is_stop]).most_common(100)\n",
    "counts = dict(counts)\n",
    "\n",
    "# Add the count to each token. \n",
    "vol_1_doc = nlp(vol_1.text)\n",
    "for token in vol_1_doc:\n",
    "    if token.lemma_ in counts.keys():\n",
    "        token._.count = counts[token.lemma_]\n",
    "\n",
    "# Repeat for the second text and find the difference \n",
    "counts = Counter([token.lemma_ for token in vol_2_doc if not token.is_punct and not token.is_stop]).most_common(100)\n",
    "counts = dict(counts)\n",
    "\n",
    "# I don't speak Hungarian, but these are clearly not words, let's get rid of them\n",
    "del counts['\\r\\n']\n",
    "del counts['\\r\\n\\r\\n']\n",
    "del counts['-e']\n",
    "\n",
    "vol_2_doc = nlp(vol_2.text)\n",
    "for token in vol_2_doc:\n",
    "    if token.lemma_ in counts.keys():\n",
    "        token._.count = counts[token.lemma_]\n",
    "\n",
    "# Now we find the difference between the most common words in the two texts        \n",
    "set_vol1 = set([(token.lemma_, token._.count) for token in vol_1_doc if token._.count])\n",
    "set_vol2 = set([(token.lemma_, token._.count) for token in vol_2_doc if token._.count])\n",
    "difference = set_vol2.difference(set_vol1)\n",
    "difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus  \n",
    "We might also want to add a dictionary definition to our vocabulary list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "                \n",
    "# PyWiktionary https://pypi.org/project/pywiktionary/\n",
    "from pywiktionary.wiktionary_parser_factory import WiktionaryParserFactory\n",
    "\n",
    "parser_factory = WiktionaryParserFactory(default_language='en')\n",
    "parser_factory_result = parser_factory.get_page('tesz')\n",
    "display(HTML(parser_factory_result['response']['query']['pages']['264347']['extract']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models \n",
    "\n",
    "What if we wanted to create a list of the 100 most freqent verbs or nouns in the text?  With the base Hungarian model, token.pos_ returns nothing. Also take a look at our lemmas. Are those really the root forms?  The basic Hungarian model simply does not know parts of speech or lemmata.  We need one that does. \n",
    "\n",
    "Here is a listing of the officially supported spaCy models: https://spacy.io/models\n",
    "There are currently models for :\n",
    "- English\n",
    "- German\n",
    "- French\n",
    "- Spanish\n",
    "- Portuguese\n",
    "- Italian\n",
    "- Dutch\n",
    "- Greek\n",
    "- Multi-language\n",
    "\n",
    "The spaCy documentation lists the features and capabilities of each model.  Keep in mind that there can be several models for a language.  Larger models are often slower and require more memory. In exchange, the larger models are often more accurate and have more features such as word vectors, dependency parsing and other pipelines.   If you're not using the more advanced features of a large model, then you would probably be better off using something small.  As a general rule, it's best to start small and then deliberately move up as needed. \n",
    "\n",
    "\n",
    "To add a spaCy supported model, simply type: \n",
    "`python -m spacy download <name of model>` `en_core_web_sm` for example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#English base language object\n",
    "#nlp = English()\n",
    "\n",
    "#English small language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "doc = nlp('Be yourself; everyone else is already taken.')\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a growing community of spaCy users.  There are dozens of spaCy-based projects in the [Universe](https://spacy.io/universe) as well as user-created language models.  If you visit [awesome-hungarian-nlp](https://github.com/oroszgy/awesome-hungarian-nlp), for example, you'll find a link to a spaCy Hungarian model [here](https://github.com/oroszgy/spacy-hungarian-models).\n",
    "\n",
    "This is a full-featured model with\n",
    "- Word vectors\n",
    "- Brown clusters\n",
    "- Token frequencies \n",
    "- Sentencizer\n",
    "- PoS Tagger\n",
    "- Lemmatizer\n",
    "- Dependency parser\n",
    "\n",
    "> If you are working locally, you'll need to install the model:  \n",
    "> `pip install https://github.com/oroszgy/spacy-hungarian-models/releases/download/hu_core_ud_lg-0.2.0/hu_core_ud_lg-0.2.0-py3-none-any.whl`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hu_core_ud_lg\n",
    "\n",
    "nlp = spacy.load('hu_core_ud_lg')\n",
    "doc = nlp('A j√∂vo sz√°zad reg√©nye.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('token: ',[token.lemma_ for token in doc])\n",
    "print('pos  : ',[token.pos_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import hu_core_ud_lg\n",
    "\n",
    "nlp = spacy.load('hu_core_ud_lg')\n",
    "\n",
    "text = '''A Szentgy√∂rgy-Duna√°gon a rakpartr√≥l egy alag√∫t visz kereszt√ºl. √Åltal√°ban\n",
    "minden√ºtt vasb√≥l √∂nt√∂tt alag√∫tak visznek √°t egyik partr√≥l a m√°sikra, mik\n",
    "k√∂nnyebben √©s gyorsabban elk√©sz√≠thet≈ëk, mint a hidak. Hidat nem\n",
    "√©p√≠tettek az Otthon v√°ros folyamain sehol, az√©rt, mert mikor a Duna nagy\n",
    "√°rad√°sban van, a haj√≥kat csak el√©j√ºk k√∂t√∂tt √¶rodromokkal lehet v√≠z\n",
    "ellen√©ben felvontatni, s ezeknek j√°r√°s√°t a hidj√°rmak akad√°lyozn√°k; hanem\n",
    "alag√∫t minden utcza nyil√°s√°b√≥l vezet √°t a t√∫lpartra. A Szentgy√∂rgy-parti\n",
    "fens√≠kon van a Duna-Delta ≈ëserdeje; most pomp√°s n√©pkertt√© √°talak√≠tva,\n",
    "melyet a vil√°g minden √©p√≠t√©si √≠zl√©se szerint alkotott ny√°ri lakok\n",
    "disz√≠tenek. Ez a ¬´f√©ny≈±z√©s v√°rosa¬ª.'''\n",
    "\n",
    "doc = nlp(text)\n",
    "verb_counter = Counter(token.lemma_ for token in doc if token.pos_ == 'VERB')\n",
    "df = pd.DataFrame(verb_counter.most_common(20))\n",
    "df.columns = ['verb', 'count']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a list of the most common new verbs we'll encounter in book 2 by adding `token.pos_=='VERB'`. Note that the large model requires 6GB of memory and cannot be run on our small virtual machine.  \n",
    "\n",
    "```python\n",
    "# Now we find the difference between the most common words in the two texts        \n",
    "set_vol1 = set([(token.lemma_, token._.count) for token in vol_1_doc if token._.count and token.pos_=='VERB'])\n",
    "set_vol2 = set([(token.lemma_, token._.count) for token in vol_2_doc if token._.count and token.pos_=='VERB'])\n",
    "difference = set_vol2.difference(set_vol1)\n",
    "difference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Reading on Parts of Speech**  \n",
    "\n",
    "[Johnathan Reeve, Isolating Literary Style with Raymond Queneau\n",
    "](https://jonreeve.com/2019/09/exercises-in-style/) ([code notebook](https://gist.github.com/JonathanReeve/cacf9d874b405b621710a7436425af49))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Named Entity Recognition\n",
    "Most of the models in spaCy have an entity recognizer.  This is similar to identifying parts of speech in the text, but greatly expands what we can automatically identify.  The types of entities and categories will vary from model to model and should be in the model's documentation. For most languages, the categories are: \n",
    "\n",
    "|   [named entities](https://spacy.io/api/annotation#ner-wikipedia-scheme) |   | \n",
    "|---|---|\n",
    "| PER  | Named person or family  |\n",
    "| ORG  | Named corporate, governmental, or other organizational entitity. |\n",
    "| LOC  | Name of politically or grographically defined location (cities, provinces, countries, international regions, bodies of water, mountains).  |\n",
    "| MISC | Miscellaneous entities, e.g. events, nationalities, products or works of art. |\n",
    "\n",
    "[Here is a list of the categories in the spaCy small English model](https://spacy.io/api/annotation#named-entities)\n",
    "\n",
    "[Here is a useful web application that can be used to assess the categories available in various spaCy models](https://explosion.ai/demos/displacy-ent)\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/usage/linguistic-features#named-entities-101).\n",
    "\n",
    "--- \n",
    "\n",
    "H.G. Wells, *The Invisible Man* (1897)\n",
    "<img src=\"https://www.slashfilm.com/wp/wp-content/images/invisible-man-cast-new.jpg\" alt=\"invisible man photo\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "invisible_man = requests.get('http://www.gutenberg.org/cache/epub/5230/pg5230.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(invisible_man.text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of people that appear in the text \n",
    "import pandas as pd\n",
    "doc = nlp(invisible_man.text)\n",
    "person_list = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        person_list.append(ent.text.replace('\\r','').replace('\\n',''))\n",
    "\n",
    "df = pd.DataFrame(set(person_list)) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of places that appear in the text \n",
    "import pandas as pd\n",
    "doc = nlp(invisible_man.text)\n",
    "place_list = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'GPE':\n",
    "        place_list.append(ent.text)\n",
    "\n",
    "df = pd.DataFrame(set(place_list)) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "displacy.render(next(doc.sents), style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source https://github.com/pmbaumgartner/binder-notebooks/blob/master/holy-nlp.ipynb \n",
    "\n",
    "actors_and_actions = []\n",
    "\n",
    "def token_is_subject_with_action(token):\n",
    "    nsubj = token.dep_ == 'nsubj'\n",
    "    head_verb = token.head.pos_ == 'VERB'\n",
    "    person = token.ent_type_ == 'PERSON'\n",
    "    return nsubj and head_verb and person\n",
    "\n",
    "for token in doc:\n",
    "    if token_is_subject_with_action(token):\n",
    "        span = doc[token.head.left_edge.i:token.head.right_edge.i+1]\n",
    "        data = dict(name=token.orth_,\n",
    "                    span=span.text,\n",
    "                    verb=token.head.lower_,\n",
    "                    log_prob=token.head.prob,\n",
    "                    )\n",
    "        actors_and_actions.append(data)\n",
    "\n",
    "print(len(actors_and_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "action_df = pd.DataFrame(actors_and_actions)\n",
    "\n",
    "print('Unique Names:', action_df['name'].nunique())\n",
    "\n",
    "most_common = (action_df\n",
    "    .groupby(['name', 'verb'])\n",
    "    .size()\n",
    "    .groupby(level=0, group_keys=False)\n",
    "    .nlargest(1)\n",
    "    .rename('Count')\n",
    "    .reset_index(level=0)\n",
    "    .rename(columns={\n",
    "        'verb': 'Most Common'\n",
    "    })\n",
    ")\n",
    "\n",
    "# exclude log prob < -20, those indicate absence in the model vocabulary\n",
    "most_unique = (action_df[action_df['log_prob'] > -20]\n",
    "    .groupby(['name', 'verb'])['log_prob']\n",
    "    .min()\n",
    "    .groupby(level=0, group_keys=False)\n",
    "    .nsmallest(1)\n",
    "    .rename('Log Prob.')\n",
    "    .reset_index(level = 0)\n",
    "    .rename(columns={\n",
    "        'verb': 'Most Unique'\n",
    "    })\n",
    ")\n",
    "\n",
    "# SO groupby credit\n",
    "# https: //stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common.sort_values('Count', ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## European Literary Text Collection (ELTeC) languages \n",
    "\n",
    "\n",
    "| Language   | spaCy base class | spaCy model                                                                               | \n",
    "|------------|------------------|-------------------------------------------------------------------------------------------| \n",
    "| Croatian   | hr               | [hr_set(spacy_stanfordnlp)](https://stanfordnlp.github.io/stanfordnlp/models.html)         | \n",
    "| Czech      | cs               | [cs_cac(spacy_stanfordnlp)](https://stanfordnlp.github.io/stanfordnlp/models.html)        | \n",
    "| Dutch      | nl               | [nl_core_news_sm](https://spacy.io/models/nl#nl_core_news_sm)                             | \n",
    "| English    | en               | [en_core_web_md](https://spacy.io/models/en#en_core_web_md)                               | \n",
    "| French     | fr               | [fr_core_news_md](https://spacy.io/models/fr#fr_core_news_md)                             | \n",
    "| German     | de               | [de_core_news_md](https://spacy.io/models/de#de_core_news_md)                              | \n",
    "| Greek      | el               | [el_core_news_md](https://spacy.io/models/el#el_core_news_md)                             | \n",
    "| Hungarian  | hu               | [hu_core_ud_lg](https://github.com/oroszgy/spacy-hungarian-models)                        | \n",
    "| Italian    | it               | [it_core_news_sm](https://spacy.io/models/it#it_core_news_sm)                             | \n",
    "| Latvian    | lv               | [lv_lvtb(spacy_stanfordnlp)](https://stanfordnlp.github.io/stanfordnlp/models.html)       | \n",
    "| Norwegan   | nb               | [no_nynorsk(spacy_stanfordnlp)](https://stanfordnlp.github.io/stanfordnlp/models.html)    | \n",
    "| Polish     | pl               | [spaCy-pl](http://spacypl.sigmoidal.io/#home)                                             | \n",
    "| Portuguese | pt               | [pt_core_news_sm](https://spacy.io/models/pt#pt_core_news_sm)                             | \n",
    "| Romanian   | ro               | [ro_rrt (spacy_stanfordnlp)](https://stanfordnlp.github.io/stanfordnlp/models.html)       | \n",
    "| Russian    | ru               | [ru_syntagrus (spacy_stanfordnlp)](https://stanfordnlp.github.io/stanfordnlp/models.html) | \n",
    "| Serbian    | sr               | [sr_set (spacy_stanfordnlp)](https://stanfordnlp.github.io/stanfordnlp/models.html)       | \n",
    "| Slovenian  | sl               | [sl_ssj (spacy_stanfordnlp)](https://stanfordnlp.github.io/stanfordnlp/models.html)       | \n",
    "| Spanish    | es               | [es_core_news_md](https://spacy.io/models/es#es_core_news_md)                             | \n",
    "| Swedish    | sv               | [ü§ò Lemmy](https://github.com/sorenlind/lemmy/)                                           | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model jƒôzyka polskiego\n",
    "import spacy\n",
    "from spacy import displacy \n",
    "\n",
    "nlp = spacy.load('pl_model')\n",
    "doc = nlp(\"BƒÖd≈∫ sobƒÖ; wszyscy inni sƒÖ ju≈º zajƒôci.\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Stanfordnlp models ![](https://pbs.twimg.com/profile_images/897182721272799232/0CplRl36_400x400.jpg)\n",
    "\n",
    "[Documentation](https://stanfordnlp.github.io/stanfordnlp/installation_usage.html)\n",
    "\n",
    "```\n",
    "$ pip install stanfordnlp spacy-stanfordnlp\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import stanfordnlp\n",
    "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "\n",
    "\n",
    "Using the default treebank \"en_ewt\" for language \"en\".\n",
    "Would you like to download the models for: en_ewt now? (Y/n)\n",
    "y\n",
    "\n",
    "Default download directory: /home/ajanco/stanfordnlp_resources\n",
    "Hit enter to continue or type an alternate directory.\n",
    "\n",
    "\n",
    "Downloading models for: en_ewt\n",
    "Download location: /home/ajanco/stanfordnlp_resources/en_ewt_models.zip\n",
    "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 235M/235M [00:51<00:00, 4.92MB/s] \n",
    "\n",
    "Download complete.  Models saved to: /home/ajanco/stanfordnlp_resources/en_ewt_models.zip\n",
    "Extracting models file for: en_ewt\n",
    "Cleaning up...Done.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "from spacy_stanfordnlp import StanfordNLPLanguage\n",
    "\n",
    "snlp = stanfordnlp.Pipeline(lang=\"en\")\n",
    "nlp = StanfordNLPLanguage(snlp)\n",
    "\n",
    "doc = nlp('Be yourself; everyone else is already taken.')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://spacy.io/architecture-bcdfffe5c0b9f221a2f6607f96ca0e4a.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language object\n",
    "from spacy.lang.es import Spanish\n",
    "nlp = Spanish()\n",
    "\n",
    "# Doc object\n",
    "doc = nlp(\"La duda es uno de los nombres de la inteligencia.\")\n",
    "\n",
    "# Tokens\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    \n",
    "# Spans\n",
    "span = doc[0:2]\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "# python -m spacy download es_core_news_sm\n",
    "\n",
    "# Models\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"La duda es uno de los nombres de la inteligencia.\")\n",
    "\n",
    "# Part of speech \n",
    "for token in doc:\n",
    "    print(token.pos_)\n",
    "    \n",
    "# Entities \n",
    "for token in doc.ents:\n",
    "    print(token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
